{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YHrPuzAvLTq"
      },
      "source": [
        "## generative models test on student lease (manually annoated data -> 500+ questions 50+ hours of work )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chrbMLdfxP76",
        "outputId": "6105d5bb-423f-430e-b91f-61d31a49d080"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylrzpWdYxYLH",
        "outputId": "b60325df-18f2-4308-d495-9b62beb58a87"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# In Google Colab, if your notebook is in /content and the files are as above:\n",
        "BASE_DIR = Path(\"/content/drive/My Drive/Dissertation/phase_2/generative\")\n",
        "AGREEMENTS_DIR = BASE_DIR / \"agreements\"\n",
        "GOLD_STANDARD_JSON = BASE_DIR / \"gold_standard.json\"\n",
        "PROCESSED_DIR = BASE_DIR / \"processed\"         # output for processed main PDFs\n",
        "IMAGES_DIR = BASE_DIR / \"images\"\n",
        "\n",
        "\n",
        "\n",
        "print(\"Base Directory:\", BASE_DIR)\n",
        "print(\"Agreements Directory:\", AGREEMENTS_DIR)\n",
        "print(\"Gold Standard JSON:\", GOLD_STANDARD_JSON)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-oJG8M-zBiC",
        "outputId": "2ec14cd0-6e46-4f3c-d478-3340e0c7be33"
      },
      "outputs": [],
      "source": [
        "pip install pymupdf4llm pdfplumber transformers rank_bm25 whoosh faiss-cpu evaluate bert-score tqdm rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnBPtw32vLTr"
      },
      "source": [
        "# IMPORTS & CONFIGURATION & PATHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jOJ2-OgHvLTs"
      },
      "outputs": [],
      "source": [
        "\n",
        "##############################\n",
        "# Imports\n",
        "##############################\n",
        "import os\n",
        "import time  # ensure time is imported\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Use your PDF-to-markdown converter; here we use pymupdf4llm as in your snippet.\n",
        "import pymupdf4llm\n",
        "\n",
        "# For alternative PDF extraction (if needed)\n",
        "import pdfplumber\n",
        "\n",
        "# For robust token counting, we use a Hugging Face tokenizer.\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# For TF-IDF retrieval:\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# For BM25 retrieval:\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "\n",
        "# For FIASS\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# For metrics:\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from bert_score import score as bertscore_score\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "# CONFIGURATION\n",
        "##############################\n",
        "\n",
        "# Maximum token limit for each chunk (set below 512 to allow room for question tokens and special tokens)\n",
        "MAX_CHUNK_TOKENS = 400\n",
        "\n",
        "# Choose a tokenizer for counting tokens (using a BERT tokenizer as an example)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mb4gnxavLTu"
      },
      "source": [
        "# PDF PREPROCESSING & MARKDOWN EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-ytdXSSnvLTv"
      },
      "outputs": [],
      "source": [
        "def pdf_to_markdown(pdf_path: Path) -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Convert a PDF to markdown using pymupdf4llm.\n",
        "    If there are issues with pymupdf4llm, consider switching to pdfplumber.\n",
        "    \"\"\"\n",
        "\n",
        "    return pymupdf4llm.to_markdown(str(pdf_path))\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Remove unwanted artifacts and non-ASCII characters.\n",
        "    \"\"\"\n",
        "\n",
        "    text = re.sub(r'â', '', text)\n",
        "    text = re.sub(r'\\*+', '', text)\n",
        "    text = re.sub(r'â*', '', text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\")\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def split_markdown_by_headers(markdown: str) -> List[Dict[str, str]]:\n",
        "\n",
        "    \"\"\"\n",
        "    Splits markdown text into sections based on headers.\n",
        "    Returns a list of dicts with keys 'title' and 'content'.\n",
        "    \"\"\"\n",
        "\n",
        "    sections = []\n",
        "    current_section = {\"title\": None, \"content\": \"\"}\n",
        "\n",
        "    for line in markdown.splitlines():\n",
        "\n",
        "        header_match = re.match(r'^(#{1,6})\\s+(.*)', line)\n",
        "        if header_match:\n",
        "            if current_section[\"title\"] is not None or current_section[\"content\"].strip():\n",
        "                sections.append(current_section)\n",
        "            title = header_match.group(2).strip()\n",
        "            current_section = {\"title\": title, \"content\": \"\"}\n",
        "        else:\n",
        "            current_section[\"content\"] += line + \"\\n\"\n",
        "\n",
        "    if current_section[\"title\"] is not None or current_section[\"content\"].strip():\n",
        "        sections.append(current_section)\n",
        "\n",
        "    return sections\n",
        "\n",
        "def process_content(text: str) -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Replace newline characters with a space and collapse extra spaces.\n",
        "    \"\"\"\n",
        "\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def preprocess_markdown_file(file_path: Path) -> List[Dict[str, str]]:\n",
        "\n",
        "    \"\"\"\n",
        "    Process a markdown file: clean, split by headers, process content,\n",
        "    and filter out trivial sections.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        markdown = f.read()\n",
        "    markdown = clean_text(markdown)\n",
        "    sections = split_markdown_by_headers(markdown)\n",
        "\n",
        "    for sec in sections:\n",
        "        sec[\"content\"] = process_content(sec[\"content\"])\n",
        "\n",
        "    # Filter out sections that are too trivial\n",
        "    filtered_sections = []\n",
        "\n",
        "    for sec in sections:\n",
        "\n",
        "        if not sec[\"content\"].strip():\n",
        "            continue\n",
        "        filtered_sections.append(sec)\n",
        "\n",
        "    return filtered_sections\n",
        "\n",
        "def save_to_json(data, filename: Path):\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def process_all_agreement_pdfs():\n",
        "\n",
        "    \"\"\"\n",
        "    Loop over all main agreement PDFs (skip Q&A PDFs that contain '_QA' in the filename)\n",
        "    and produce a JSON file per agreement.\n",
        "    \"\"\"\n",
        "\n",
        "    for pdf_file in AGREEMENTS_DIR.glob(\"Agreement_*.pdf\"):\n",
        "\n",
        "        if \"_QA\" in pdf_file.stem:\n",
        "            continue  # Skip Q&A PDFs\n",
        "        print(f\"Processing {pdf_file.name} ...\")\n",
        "\n",
        "        # Convert PDF to markdown text.\n",
        "        md_text = pdf_to_markdown(pdf_file)\n",
        "\n",
        "        # Save the markdown to the output directory using the correct path.\n",
        "        output_md_path = AGREEMENTS_DIR / f\"{pdf_file.stem}.md\"\n",
        "        output_md_path.write_text(md_text, encoding=\"utf-8\")\n",
        "        print(f\"Saved markdown to {output_md_path}\")\n",
        "\n",
        "        sections = preprocess_markdown_file(pdf_file.with_suffix(\".md\"))\n",
        "        # In case you haven't already saved markdown to disk, you can also do:\n",
        "        # sections = split_markdown_by_headers(clean_text(md_text))\n",
        "        # Save processed sections to JSON:\n",
        "        output_json = PROCESSED_DIR / f\"{pdf_file.stem}.json\"\n",
        "        save_to_json(sections, output_json)\n",
        "\n",
        "\n",
        "##############################\n",
        "# FURTHER CHUNKING USING TOKEN COUNTS\n",
        "##############################\n",
        "\n",
        "def chunk_section_by_tokens(section: Dict[str, str], max_tokens: int = MAX_CHUNK_TOKENS) -> List[Dict[str, str]]:\n",
        "\n",
        "    \"\"\"\n",
        "    Use the Hugging Face tokenizer to count tokens and split a section's content into sub‐chunks.\n",
        "    The method splits on sentence boundaries if possible.\n",
        "    \"\"\"\n",
        "\n",
        "    text = section[\"content\"]\n",
        "\n",
        "    # Tokenize using the model's tokenizer (which returns token IDs)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return [section]\n",
        "\n",
        "    # For a better split, we can try to split by sentences.\n",
        "    # Here we use a naive regex sentence split; you might also use nltk.sent_tokenize.\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    current_tokens = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_tokens = tokenizer.tokenize(sent)\n",
        "\n",
        "        # If adding the sentence exceeds max_tokens, store the current chunk.\n",
        "        if len(current_tokens) + len(sent_tokens) > max_tokens:\n",
        "            if current_chunk:\n",
        "                chunks.append({\n",
        "                    \"title\": section[\"title\"],\n",
        "                    \"content\": current_chunk.strip()\n",
        "                })\n",
        "            # Start a new chunk with this sentence.\n",
        "            current_chunk = sent + \" \"\n",
        "            current_tokens = sent_tokens\n",
        "        else:\n",
        "            current_chunk += sent + \" \"\n",
        "            current_tokens += sent_tokens\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append({\n",
        "            \"title\": section[\"title\"],\n",
        "            \"content\": current_chunk.strip()\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def further_chunk_sections(sections: List[Dict[str, str]], max_tokens: int = MAX_CHUNK_TOKENS) -> List[Dict[str, str]]:\n",
        "\n",
        "    \"\"\"\n",
        "    Apply token-based chunking to all sections.\n",
        "    \"\"\"\n",
        "\n",
        "    final_chunks = []\n",
        "\n",
        "    for sec in sections:\n",
        "        sub_chunks = chunk_section_by_tokens(sec, max_tokens=max_tokens)\n",
        "        final_chunks.extend(sub_chunks)\n",
        "\n",
        "    return final_chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnFcq2DgvLTw"
      },
      "source": [
        "# GOLD STANDARD Q&A EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ddNasNp8vLTx"
      },
      "outputs": [],
      "source": [
        "def save_to_json_QA(data, filename):\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Gold standard JSON saved to {filename}\")\n",
        "\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "\n",
        "    \"\"\"Extract text from a PDF file using pdfplumber.\"\"\"\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_qa_pairs(text):\n",
        "\n",
        "    \"\"\"\n",
        "    Extract Q&A pairs from text.\n",
        "    Assumes a format where each pair starts with 'Question <number>:'\n",
        "    and then 'Answer :' with the answer continuing until the next question or end-of-text.\n",
        "    \"\"\"\n",
        "\n",
        "    qa_pattern = re.compile(\n",
        "        r\"Question\\s*(\\d+)\\s*:\\s*(.*?)\\s*Answer\\s*:\\s*(.*?)(?=Question\\s*\\d+\\s*:|$)\",\n",
        "        re.DOTALL | re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    qa_pairs = []\n",
        "\n",
        "    for match in qa_pattern.finditer(text):\n",
        "\n",
        "        question_num, question, answer = match.groups()\n",
        "        qa_pairs.append({\n",
        "            \"question_number\": question_num.strip(),\n",
        "            \"question\": question.strip(),\n",
        "            \"answer\": answer.strip()\n",
        "        })\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "\n",
        "def build_gold_standard():\n",
        "\n",
        "    \"\"\"\n",
        "    Loop over all Q&A PDFs and build a dictionary.\n",
        "    Save as a single JSON file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Built a dictionary to hold the gold standard for all agreements\n",
        "    gold_standard = {}\n",
        "\n",
        "    # Looping over all Q&A PDFs in the agreements directory\n",
        "\n",
        "    for pdf_file in AGREEMENTS_DIR.glob(\"*_QA.pdf\"):\n",
        "\n",
        "        # Extracting an agreement identifier from the filename, \"Agreement_N\"\n",
        "        agreement_id = pdf_file.stem.split(\"_QA\")[0]\n",
        "        print(f\"Processing Q&A for {agreement_id} from {pdf_file.name}\")\n",
        "\n",
        "        # Extract text and then Q&A pairs using pdfplumber\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "        qa_pairs = extract_qa_pairs(text)\n",
        "\n",
        "        # Store the result in the gold standard dictionary\n",
        "        gold_standard[agreement_id] = qa_pairs\n",
        "\n",
        "    # Order the dictionary by the numeric part of the agreement id.\n",
        "    # Assuming agreement IDs are in the form \"Agreement_<number>\"\n",
        "    ordered_gold_standard = dict(\n",
        "\n",
        "        sorted(\n",
        "            gold_standard.items(),\n",
        "            key=lambda x: int(x[0].split('_')[1]) if x[0].split('_')[1].isdigit() else 0\n",
        "        )\n",
        "\n",
        "    )\n",
        "\n",
        "    save_to_json(ordered_gold_standard, GOLD_STANDARD_JSON)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAVncFVUvLTx"
      },
      "source": [
        "# GENERTAIVE RETRIEVAL FUNCTIONS (FAISS only and FAISS + BM25 Hybrid.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CYQxrtrevLTy"
      },
      "outputs": [],
      "source": [
        "class FaissRetriever:\n",
        "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.embedder = SentenceTransformer(model_name)\n",
        "        self.index = None\n",
        "        self.chunk_texts = None\n",
        "\n",
        "    def build_index(self, chunks: List[Dict[str, str]]):\n",
        "        self.chunk_texts = [chunk[\"content\"] for chunk in chunks]\n",
        "        embeddings = self.embedder.encode(self.chunk_texts, convert_to_numpy=True)\n",
        "        dim = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dim)\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3) -> List[Dict[str, any]]:\n",
        "        q_embed = self.embedder.encode([query], convert_to_numpy=True)\n",
        "        faiss.normalize_L2(q_embed)\n",
        "        distances, indices = self.index.search(q_embed, top_k)\n",
        "        results = []\n",
        "        for dist, idx in zip(distances[0], indices[0]):\n",
        "            results.append({\"text\": self.chunk_texts[idx], \"score\": float(dist)})\n",
        "        return results\n",
        "\n",
        "class BM25Retriever:\n",
        "    # Simple BM25 retrieval based on tokenized content.\n",
        "    def index(self, chunks: List[Dict[str, str]]):\n",
        "        from rank_bm25 import BM25Okapi\n",
        "        self.docs = [chunk[\"content\"] for chunk in chunks]\n",
        "        self.tokenized_docs = [doc.split() for doc in self.docs]  # very basic tokenization\n",
        "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
        "    \n",
        "    def search(self, query: str, top_k: int = 3) -> List[Dict[str, any]]:\n",
        "        query_tokens = query.split()\n",
        "        scores = self.bm25.get_scores(query_tokens)\n",
        "        ranked_idx = np.argsort(scores)[::-1][:top_k]\n",
        "        results = [{\"text\": self.docs[idx], \"score\": float(scores[idx])} for idx in ranked_idx]\n",
        "        return results\n",
        "\n",
        "class HybridRetriever:\n",
        "    \"\"\"\n",
        "    Combine FAISS and BM25 scores by taking a simple average of the normalized scores.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.faiss_retriever = FaissRetriever()\n",
        "        self.bm25_retriever = BM25Retriever()\n",
        "    \n",
        "    def index(self, chunks: List[Dict[str, str]]):\n",
        "        self.faiss_retriever.build_index(chunks)\n",
        "        self.bm25_retriever.index(chunks)\n",
        "        # Save chunks for later (they should be the same for both methods)\n",
        "        self.chunk_texts = [chunk[\"content\"] for chunk in chunks]\n",
        "    \n",
        "    def search(self, query: str, top_k: int = 3) -> List[Dict[str, any]]:\n",
        "        # Retrieve using FAISS and BM25.\n",
        "        faiss_results = self.faiss_retriever.search(query, top_k)\n",
        "        bm25_results = self.bm25_retriever.search(query, top_k)\n",
        "        # Normalize scores (we assume scores are between 0 and 1 for FAISS, but BM25 might need normalization)\n",
        "        # Here, we take a simple average based on rank positions.\n",
        "        combined = []\n",
        "        for i in range(top_k):\n",
        "            # If one method returns fewer than top_k, use 0 for missing scores.\n",
        "            faiss_score = faiss_results[i][\"score\"] if i < len(faiss_results) else 0\n",
        "            bm25_score = bm25_results[i][\"score\"] if i < len(bm25_results) else 0\n",
        "            avg_score = (faiss_score + bm25_score) / 2.0\n",
        "            # For context, choose the one with higher individual score (or you could choose to combine texts)\n",
        "            best_text = faiss_results[i][\"text\"] if faiss_score >= bm25_score else bm25_results[i][\"text\"]\n",
        "            combined.append({\"text\": best_text, \"score\": avg_score})\n",
        "        return combined\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgNbZW4YvLTz"
      },
      "source": [
        "# GENERATIVE MODELS LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WL45k_wavLTz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_generative_pipelines():\n",
        "    \n",
        "    return {\n",
        "        \"mistralai\": pipeline(\"text2text-generation\", \n",
        "                              model=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
        "                              device=0),\n",
        "\n",
        "        \"legal_llama\": pipeline(\"text2text-generation\", \n",
        "                                model=\"simmo/legal-llama-3\", \n",
        "                                device=0)\n",
        "\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeqDecWwvLT0"
      },
      "source": [
        "# RUNNING THE RETRIEVAL & QA EXPERIMENT & EVALUATION METRICS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "5teeMROvvLT0"
      },
      "outputs": [],
      "source": [
        "# ----- Experiment for Generative Models -----\n",
        "def run_gen_experiments(gold_data, processed_docs, gen_pipelines, retriever_dict, top_k=3):\n",
        "    \"\"\"\n",
        "    For each agreement, for each Q&A pair from the gold standard:\n",
        "      - Retrieve top context chunks using each retrieval strategy.\n",
        "      - For each retrieval strategy, generate an answer using the generative model.\n",
        "    In this experiment we test two retrieval strategies:\n",
        "      1. FAISS only.\n",
        "      2. Hybrid (FAISS + BM25).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for agreement_id, qa_pairs in tqdm(gold_data.items(), desc=\"Processing Agreements\"):\n",
        "\n",
        "        if agreement_id not in processed_docs:\n",
        "            print(f\"Warning: No processed chunks for {agreement_id}\")\n",
        "\n",
        "            continue\n",
        "\n",
        "        chunks = processed_docs[agreement_id]\n",
        "        \n",
        "        # Skip if chunks are empty or contain no useful text.\n",
        "        if not chunks or all(not re.search(r'\\w', chunk.get(\"content\", \"\")) for chunk in chunks):\n",
        "            print(f\"Skipping {agreement_id} due to empty content.\")\n",
        "            continue\n",
        "\n",
        "        # Further chunk if needed.\n",
        "        chunks = further_chunk_sections(chunks, max_tokens=400)\n",
        "        \n",
        "        for retrieval_strategy, retriever in retriever_dict.items():\n",
        "\n",
        "\n",
        "            # For each retrieval strategy, index the chunks.\n",
        "            retriever.index(chunks)\n",
        "\n",
        "            for qa in qa_pairs:\n",
        "\n",
        "                question = qa[\"question\"]\n",
        "                gold_answer = qa[\"answer\"]\n",
        "                start_time = time.time()\n",
        "\n",
        "                retrieved = retriever.search(question, top_k=top_k)\n",
        "\n",
        "                # Combine the retrieved context into a single prompt.\n",
        "                context = \" \".join([item[\"text\"] for item in retrieved])\n",
        "\n",
        "                # Create a prompt: include the question and the context.\n",
        "\n",
        "                prompt = (\n",
        "                            \"Answer the question in one or two sentences and be direct as possible.\\n\"\n",
        "                            \"Do not repeat the question or the context; only provide the final answer.\\n\"\n",
        "                            f\"Question: {question}\\n\"\n",
        "                            f\"Context: {context}\\n\"\n",
        "                            \"Final Answer:\"\n",
        "                        )\n",
        "\n",
        "                # For each generative model, generate an answer.\n",
        "                for model_name, gen_pipeline in gen_pipelines.items():\n",
        "                    gen_start = time.time()\n",
        "\n",
        "                    # Generate output; adjust max_length and other parameters as needed.\n",
        "                    gen_output = gen_pipeline(prompt, max_length=200, truncation=True)\n",
        "\n",
        "                    elapsed_model = time.time() - gen_start\n",
        "                    # Assume the generated text is in gen_output[0]['generated_text']\n",
        "                    pred_answer = gen_output[0]['generated_text'].strip()\n",
        "                    total_time = time.time() - start_time\n",
        "\n",
        "                    results.append({\n",
        "                        \"retrieval_strategy\": retrieval_strategy,\n",
        "                        \"Model\": model_name,\n",
        "                        \"agreement_id\": agreement_id,\n",
        "                        \"question\": question,\n",
        "                        \"gold_answer\": gold_answer,\n",
        "                        \"pred_answer\": pred_answer,\n",
        "                        \"time\": total_time,\n",
        "                        \"model_time\": elapsed_model  # generation time only\n",
        "                    })\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "#EVALUATION METRICS\n",
        "##############################\n",
        "\n",
        "def normalize_text(s):\n",
        "\n",
        "    s = s.lower().strip()\n",
        "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def tokenize_text(s):\n",
        "\n",
        "    return normalize_text(s).split()\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_gen(results: List[Dict[str, str]]) -> Dict:\n",
        "    from collections import defaultdict\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    grouped = defaultdict(lambda: {\"preds\": [], \"golds\": [], \"times\": []})\n",
        "    \n",
        "    for res in results:\n",
        "        key = (res[\"retrieval_strategy\"], res[\"Model\"])\n",
        "        grouped[key][\"preds\"].append(res[\"pred_answer\"])\n",
        "        grouped[key][\"golds\"].append(res[\"gold_answer\"])\n",
        "        grouped[key][\"times\"].append(res.get(\"time\", 0))\n",
        "    \n",
        "    final = {}\n",
        "    for key, data in grouped.items():\n",
        "        # Compute ROUGE-L for generative answers.\n",
        "        rouge_scores = rouge.compute(predictions=data[\"preds\"], references=data[\"golds\"])\n",
        "        avg_rouge_l = rouge_scores[\"rougeL\"]\n",
        "        \n",
        "        # Compute BERTScore as semantic similarity measure.\n",
        "        from bert_score import score as bertscore_score\n",
        "        P, R, F = bertscore_score(data[\"preds\"], data[\"golds\"], lang=\"en\")\n",
        "        avg_bertscore = float(torch.mean(F))\n",
        "        avg_time = np.mean(data[\"times\"])\n",
        "        final[key] = {\n",
        "            \"ROUGE-L\": avg_rouge_l,\n",
        "            \"BERTScore\": avg_bertscore,\n",
        "            \"Avg Time\": avg_time\n",
        "        }\n",
        "    return final\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlNqUiPbvLT0"
      },
      "source": [
        "# MAIN EXECUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "isGzuDgyvLT1",
        "outputId": "1790d8ec-504e-4119-af0d-5ee58e065924"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----- Main Function for Generative Experiment -----\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_aggregated_metrics(df_agg, images_dir):\n",
        "    # Select only the numeric columns (or explicitly define them)\n",
        "    metric_columns = [\"Exact Match\", \"F1\", \"Partial F1\", \"ROUGE-L\", \"BERTScore\", \"Semantic Similarity\", \"Avg Time\", \"Avg Confidence\"]\n",
        "    # Group by Model and average over retrievers.\n",
        "    models_avg = df_agg.groupby(\"Model\")[metric_columns].mean().reset_index()\n",
        "\n",
        "    # Reshape the data for plotting\n",
        "    df_models = pd.melt(models_avg, id_vars=[\"Model\"], value_vars=metric_columns,\n",
        "                        var_name=\"Metric\", value_name=\"Value\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for metric in metric_columns:\n",
        "        subset = df_models[df_models[\"Metric\"] == metric]\n",
        "        plt.bar(subset[\"Model\"] + \" (\" + metric + \")\", subset[\"Value\"])\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.title(\"Aggregated Metrics per QA Model (Averaged over Retrievers)\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(images_dir / \"chart_aggregated_metrics.png\", dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_metrics_grouped_by_model(df_agg, images_dir):\n",
        "    # Define the list of metrics you want to plot.\n",
        "    metrics_list = [\"Exact Match\", \"F1\", \"Partial F1\", \"ROUGE-L\", \"BERTScore\", \"Semantic Similarity\", \"Avg Time\", \"Avg Confidence\"]\n",
        "    \n",
        "    # Create one subplot per metric.\n",
        "    n_metrics = len(metrics_list)\n",
        "    fig, axes = plt.subplots(n_metrics, 1, figsize=(10, 4 * n_metrics))\n",
        "    \n",
        "    for i, metric in enumerate(metrics_list):\n",
        "        # Pivot the DataFrame: rows = Model, columns = Retriever, values = the metric.\n",
        "        pivot = df_agg.pivot(index=\"Model\", columns=\"Retriever\", values=metric)\n",
        "        \n",
        "        # Plot the pivot table as a grouped bar chart.\n",
        "        pivot.plot(kind=\"bar\", ax=axes[i])\n",
        "        axes[i].set_title(f\"{metric} by Model and Retrieval Strategy\")\n",
        "        axes[i].set_xlabel(\"Model\")\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].legend(title=\"Retriever\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(images_dir / \"chart_grouped_by_model.png\", dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_pivoted_metrics(df_agg, images_dir):\n",
        "    metrics_list = [\"Exact Match\", \"F1\", \"Partial F1\", \"ROUGE-L\", \"BERTScore\", \"Semantic Similarity\", \"Avg Time\", \"Avg Confidence\"]\n",
        "    n_metrics = len(metrics_list)\n",
        "    n_cols = 2\n",
        "    n_rows = (n_metrics + n_cols - 1) // n_cols  # ceiling division\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 4))\n",
        "    axes = axes.flatten()\n",
        "    for i, metric in enumerate(metrics_list):\n",
        "        pivot = df_agg.pivot(index=\"Retriever\", columns=\"Model\", values=metric)\n",
        "        pivot.plot(kind=\"bar\", ax=axes[i])\n",
        "        axes[i].set_title(f\"Average {metric}: QA Models vs. Retrieval Methods\")\n",
        "        axes[i].set_xlabel(\"Retriever\")\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].legend(title=\"Model\")\n",
        "        axes[i].set_xticklabels(pivot.index, rotation=0)\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(images_dir / \"chart_pivoted_metrics.png\", dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "def plot_processing_time_line(df_detailed, images_dir):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    for (retriever, model), subset in df_detailed.groupby([\"retriever\", \"Model\"]):\n",
        "        label = f\"{retriever}-{model}\"\n",
        "        plt.plot(subset.index, subset[\"time\"], marker=\"o\", label=label)\n",
        "    plt.title(\"Time per QA Example by Retriever-Model Combination\")\n",
        "    plt.xlabel(\"Example Index\")\n",
        "    plt.ylabel(\"Time (seconds)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(images_dir / \"chart_processing_time_line.png\", dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter_with_best_fit(df, x_col, y_col, title, xlabel, ylabel, save_filename, images_dir):\n",
        "    x = df[x_col].values\n",
        "    y = df[y_col].values\n",
        "    coeffs = np.polyfit(x, y, 1)  # Linear regression coefficients\n",
        "    line_x = np.linspace(x.min(), x.max(), 100)\n",
        "    line_y = np.polyval(coeffs, line_x)\n",
        "    \n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.scatter(x, y, alpha=0.6, label=\"Data points\")\n",
        "    plt.plot(line_x, line_y, color='red', label=\"Best-fit line\")\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(images_dir / save_filename, dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_boxplot_time(df_detailed, images_dir):\n",
        "    df_detailed['Combination'] = df_detailed['retriever'] + \"-\" + df_detailed['Model']\n",
        "    combinations = df_detailed['Combination'].unique()\n",
        "    data = [df_detailed[df_detailed['Combination'] == comb]['time'].values for comb in combinations]\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.boxplot(data, labels=combinations, patch_artist=True)\n",
        "    plt.xlabel(\"Retriever-Model Combination\")\n",
        "    plt.ylabel(\"Time (seconds)\")\n",
        "    plt.title(\"Box Plot: Processing Time per Combination\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(images_dir / \"boxplot_time.png\", dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_cdf_time(df_detailed, images_dir):\n",
        "    sorted_time = np.sort(df_detailed['time'].values)\n",
        "    cdf = np.arange(len(sorted_time)) / float(len(sorted_time))\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(sorted_time, cdf, marker=\".\", linestyle=\"none\")\n",
        "    plt.xlabel(\"Time (seconds)\")\n",
        "    plt.ylabel(\"CDF\")\n",
        "    plt.title(\"CDF of Processing Time for Q&A Examples\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(images_dir / \"cdf_time.png\", dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    # 1. Preprocess and build gold standard as before.\n",
        "    process_all_agreement_pdfs()\n",
        "\n",
        "    \n",
        "    build_gold_standard()\n",
        "    \n",
        "    with open(GOLD_STANDARD_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "        gold_data = json.load(f)\n",
        "\n",
        "\n",
        "    print(\"Gold data keys:\", list(gold_data.keys()))\n",
        "    \n",
        "    processed_docs = {}\n",
        "\n",
        "\n",
        "    for json_file in PROCESSED_DIR.glob(\"Agreement_*.json\"):\n",
        "        agreement_id = json_file.stem\n",
        "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            processed_docs[agreement_id] = json.load(f)\n",
        "    \n",
        "    # 2. Load generative QA pipelines.\n",
        "    gen_pipelines = load_generative_pipelines()\n",
        "    \n",
        "    # 3. Initialize retrieval strategies:\n",
        "\n",
        "    # Strategy 1: FAISS only.\n",
        "    faiss_only = FaissRetriever()\n",
        "\n",
        "\n",
        "    # Strategy 2: Hybrid: FAISS + BM25.\n",
        "    hybrid = HybridRetriever()\n",
        "    \n",
        "    retrieval_strategies = {\n",
        "        \"faiss\": faiss_only,\n",
        "        \"hybrid\": hybrid\n",
        "    }\n",
        "    \n",
        "    # 4. Run experiments using the generative models.\n",
        "    results = run_gen_experiments(gold_data, processed_docs, gen_pipelines, retrieval_strategies, top_k=3)\n",
        "    print(\"Generated results keys:\", results[0].keys())\n",
        "    \n",
        "    # Save detailed results.\n",
        "    df_detailed = pd.DataFrame(results)\n",
        "    detailed_csv_path = BASE_DIR / \"detailed_results_gen.csv\"\n",
        "    df_detailed.to_csv(detailed_csv_path, index=False)\n",
        "    print(f\"Detailed generative results saved to {detailed_csv_path}\")\n",
        "    \n",
        "\n",
        "    # 5. Evaluate the generative experiment.\n",
        "    metrics = evaluate_gen(results)\n",
        "    metric_rows = []\n",
        "\n",
        "    for (strategy, model), scores in metrics.items():\n",
        "        row = {\"Retrieval Strategy\": strategy, \"Model\": model}\n",
        "        row.update(scores)\n",
        "        metric_rows.append(row)\n",
        "\n",
        "\n",
        "    df_agg = pd.DataFrame(metric_rows)\n",
        "    agg_csv_path = BASE_DIR / \"aggregated_metrics_gen.csv\"\n",
        "    df_agg.to_csv(agg_csv_path, index=False)\n",
        "    print(f\"Aggregated generative metrics saved to {agg_csv_path}\")\n",
        "    \n",
        "    print(df_agg.dtypes)\n",
        "    print(df_agg.head())\n",
        "\n",
        "\n",
        "\n",
        "    plot_aggregated_metrics(df_agg, IMAGES_DIR)\n",
        "    plot_metrics_grouped_by_model(df_agg, IMAGES_DIR)\n",
        "    plot_pivoted_metrics(df_agg, IMAGES_DIR)\n",
        "    plot_processing_time_line(df_detailed, IMAGES_DIR)\n",
        "\n",
        "    plot_scatter_with_best_fit(df_detailed, \"confidence\", \"BERTScore\",\n",
        "                           \"Scatter Plot: BERTScore vs Confidence with Best-fit Line\",\n",
        "                           \"Confidence Score\", \"BERTScore\", \"scatter_bert_sem_sim_conf.png\", IMAGES_DIR)\n",
        "\n",
        "    plot_scatter_with_best_fit(df_detailed, \"confidence\", \"Semantic Similarity\",\n",
        "                            \"Scatter Plot: Semantic Similarity vs Confidence with Best-fit Line\",\n",
        "                            \"Confidence Score\", \"Semantic Similarity\", \"scatter_sem_sim_conf.png\", IMAGES_DIR)\n",
        "\n",
        "    plot_scatter_with_best_fit(df_detailed, \"confidence\", \"F1\",\n",
        "                            \"Scatter Plot: F1 vs Confidence with Best-fit Line\",\n",
        "                            \"Confidence Score\", \"F1 Score\", \"scatter_f1_conf.png\", IMAGES_DIR)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhtuiYzOvLT1"
      },
      "source": [
        "# Download all files"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
