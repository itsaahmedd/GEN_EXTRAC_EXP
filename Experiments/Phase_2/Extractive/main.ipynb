{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extractive models test on student lease (manually annoated data -> 500+ questions 50+ hours of work )\n",
    "## total lines of code: 929"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS & CONFIGURATION & PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################\n",
    "# Imports\n",
    "##############################\n",
    "import os\n",
    "import time  # ensure time is imported\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use your PDF-to-markdown converter; here we use pymupdf4llm as in your snippet.\n",
    "import pymupdf4llm\n",
    "\n",
    "# For alternative PDF extraction (if needed)\n",
    "import pdfplumber\n",
    "\n",
    "# For robust token counting, we use a Hugging Face tokenizer.\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# For TF-IDF retrieval:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# For BM25 retrieval:\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# For Woosh\n",
    "\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# For FIASS\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransforme\n",
    "\n",
    "\n",
    "# For metrics:\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from bert_score import score as bertscore_score\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# CONFIGURATION & PATHS\n",
    "##############################\n",
    "\n",
    "# Directories (adjust as needed)\n",
    "BASE_DIR = Path.cwd().parent    # e.g., phase_2 folder\n",
    "AGREEMENTS_DIR = BASE_DIR / \"agreements\"      # contains both main PDFs and Q&A PDFs\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"         # output for processed main PDFs\n",
    "GOLD_STANDARD_JSON = BASE_DIR / \"gold_standard.json\"\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Maximum token limit for each chunk (set below 512 to allow room for question tokens and special tokens)\n",
    "MAX_CHUNK_TOKENS = 400\n",
    "\n",
    "# Choose a tokenizer for counting tokens (using a BERT tokenizer as an example)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF PREPROCESSING & MARKDOWN EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_markdown(pdf_path: Path) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Convert a PDF to markdown using pymupdf4llm.\n",
    "    If there are issues with pymupdf4llm, consider switching to pdfplumber.\n",
    "    \"\"\"\n",
    "\n",
    "    return pymupdf4llm.to_markdown(str(pdf_path))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Remove unwanted artifacts and non-ASCII characters.\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'â', '', text)\n",
    "    text = re.sub(r'\\*+', '', text)\n",
    "    text = re.sub(r'â*', '', text)\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\")\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def split_markdown_by_headers(markdown: str) -> List[Dict[str, str]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Splits markdown text into sections based on headers.\n",
    "    Returns a list of dicts with keys 'title' and 'content'.\n",
    "    \"\"\"\n",
    "\n",
    "    sections = []\n",
    "    current_section = {\"title\": None, \"content\": \"\"}\n",
    "\n",
    "    for line in markdown.splitlines():\n",
    "\n",
    "        header_match = re.match(r'^(#{1,6})\\s+(.*)', line)\n",
    "        if header_match:\n",
    "            if current_section[\"title\"] is not None or current_section[\"content\"].strip():\n",
    "                sections.append(current_section)\n",
    "            title = header_match.group(2).strip()\n",
    "            current_section = {\"title\": title, \"content\": \"\"}\n",
    "        else:\n",
    "            current_section[\"content\"] += line + \"\\n\"\n",
    "\n",
    "    if current_section[\"title\"] is not None or current_section[\"content\"].strip():\n",
    "        sections.append(current_section)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def process_content(text: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Replace newline characters with a space and collapse extra spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_markdown_file(file_path: Path) -> List[Dict[str, str]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Process a markdown file: clean, split by headers, process content,\n",
    "    and filter out trivial sections.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        markdown = f.read()\n",
    "    markdown = clean_text(markdown)\n",
    "    sections = split_markdown_by_headers(markdown)\n",
    "\n",
    "    for sec in sections:\n",
    "        sec[\"content\"] = process_content(sec[\"content\"])\n",
    "\n",
    "    # Filter out sections that are too trivial or too long (over 1500 words, for instance)\n",
    "    filtered_sections = []\n",
    "    \n",
    "    for sec in sections:\n",
    "        word_count = len(sec[\"content\"].split())\n",
    "        if not sec[\"content\"].strip() or word_count > 1500:\n",
    "            continue\n",
    "        filtered_sections.append(sec)\n",
    "\n",
    "    return filtered_sections\n",
    "\n",
    "def save_to_json(data, filename: Path):\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def process_all_agreement_pdfs():\n",
    "\n",
    "    \"\"\"\n",
    "    Loop over all main agreement PDFs (skip Q&A PDFs that contain '_QA' in the filename)\n",
    "    and produce a JSON file per agreement.\n",
    "    \"\"\"\n",
    "\n",
    "    for pdf_file in AGREEMENTS_DIR.glob(\"Agreement_*.pdf\"):\n",
    "\n",
    "        if \"_QA\" in pdf_file.stem:\n",
    "            continue  # Skip Q&A PDFs\n",
    "        print(f\"Processing {pdf_file.name} ...\")\n",
    "        \n",
    "        md_text = pdf_to_markdown(pdf_file)\n",
    "        # Optionally save the raw markdown if needed:\n",
    "        # pathlib.Path(\"output.md\").write_text(md_text, encoding=\"utf-8\")\n",
    "        sections = preprocess_markdown_file(pdf_file.with_suffix(\".md\"))\n",
    "        # In case you haven't already saved markdown to disk, you can also do:\n",
    "        # sections = split_markdown_by_headers(clean_text(md_text))\n",
    "        # Save processed sections to JSON:\n",
    "        output_json = PROCESSED_DIR / f\"{pdf_file.stem}.json\"\n",
    "        save_to_json(sections, output_json)\n",
    "\n",
    "\n",
    "##############################\n",
    "# FURTHER CHUNKING USING TOKEN COUNTS\n",
    "##############################\n",
    "\n",
    "def chunk_section_by_tokens(section: Dict[str, str], max_tokens: int = MAX_CHUNK_TOKENS) -> List[Dict[str, str]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Use the Hugging Face tokenizer to count tokens and split a section's content into sub‐chunks.\n",
    "    The method splits on sentence boundaries if possible.\n",
    "    \"\"\"\n",
    "\n",
    "    text = section[\"content\"]\n",
    "\n",
    "    # Tokenize using the model's tokenizer (which returns token IDs)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [section]\n",
    "    \n",
    "    # For a better split, we can try to split by sentences.\n",
    "    # Here we use a naive regex sentence split; you might also use nltk.sent_tokenize.\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_tokens = tokenizer.tokenize(sent)\n",
    "\n",
    "        # If adding the sentence exceeds max_tokens, store the current chunk.\n",
    "        if len(current_tokens) + len(sent_tokens) > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append({\n",
    "                    \"title\": section[\"title\"],\n",
    "                    \"content\": current_chunk.strip()\n",
    "                })\n",
    "            # Start a new chunk with this sentence.\n",
    "            current_chunk = sent + \" \"\n",
    "            current_tokens = sent_tokens\n",
    "        else:\n",
    "            current_chunk += sent + \" \"\n",
    "            current_tokens += sent_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append({\n",
    "            \"title\": section[\"title\"],\n",
    "            \"content\": current_chunk.strip()\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def further_chunk_sections(sections: List[Dict[str, str]], max_tokens: int = MAX_CHUNK_TOKENS) -> List[Dict[str, str]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Apply token-based chunking to all sections.\n",
    "    \"\"\"\n",
    "\n",
    "    final_chunks = []\n",
    "\n",
    "    for sec in sections:\n",
    "        sub_chunks = chunk_section_by_tokens(sec, max_tokens=max_tokens)\n",
    "        final_chunks.extend(sub_chunks)\n",
    "\n",
    "    return final_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOLD STANDARD Q&A EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json_QA(data, filename):\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Gold standard JSON saved to {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "\n",
    "    \"\"\"Extract text from a PDF file using pdfplumber.\"\"\"\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_qa_pairs(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract Q&A pairs from text.\n",
    "    Assumes a format where each pair starts with 'Question <number>:' \n",
    "    and then 'Answer :' with the answer continuing until the next question or end-of-text.\n",
    "    \"\"\"\n",
    "\n",
    "    qa_pattern = re.compile(\n",
    "        r\"Question\\s*(\\d+)\\s*:\\s*(.*?)\\s*Answer\\s*:\\s*(.*?)(?=Question\\s*\\d+\\s*:|$)\",\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    qa_pairs = []\n",
    "\n",
    "    for match in qa_pattern.finditer(text):\n",
    "\n",
    "        question_num, question, answer = match.groups()\n",
    "        qa_pairs.append({\n",
    "            \"question_number\": question_num.strip(),\n",
    "            \"question\": question.strip(),\n",
    "            \"answer\": answer.strip()\n",
    "        })\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def build_gold_standard():\n",
    "\n",
    "    \"\"\"\n",
    "    Loop over all Q&A PDFs and build a dictionary.\n",
    "    Save as a single JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Built a dictionary to hold the gold standard for all agreements\n",
    "    gold_standard = {}\n",
    "\n",
    "    # Looping over all Q&A PDFs in the agreements directory\n",
    "\n",
    "    for pdf_file in AGREEMENTS_DIR.glob(\"*_QA.pdf\"):\n",
    "\n",
    "        # Extracting an agreement identifier from the filename, \"Agreement_N\"\n",
    "        agreement_id = pdf_file.stem.split(\"_QA\")[0]\n",
    "        print(f\"Processing Q&A for {agreement_id} from {pdf_file.name}\")\n",
    "\n",
    "        # Extract text and then Q&A pairs using pdfplumber\n",
    "        text = extract_text_from_pdf(pdf_file)\n",
    "        qa_pairs = extract_qa_pairs(text)\n",
    "\n",
    "        # Store the result in the gold standard dictionary\n",
    "        gold_standard[agreement_id] = qa_pairs\n",
    "\n",
    "    # Order the dictionary by the numeric part of the agreement id.\n",
    "    # Assuming agreement IDs are in the form \"Agreement_<number>\"\n",
    "    ordered_gold_standard = dict(\n",
    "\n",
    "        sorted(\n",
    "            gold_standard.items(),\n",
    "            key=lambda x: int(x[0].split('_')[1]) if x[0].split('_')[1].isdigit() else 0\n",
    "        )\n",
    "\n",
    "    )\n",
    "\n",
    "    save_to_json(ordered_gold_standard, GOLD_STANDARD_JSON)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVAL FUNCTIONS (TF-IDF & BM25 & WHOOSH & FIASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhooshRetriever:\n",
    "    def __init__(self):\n",
    "        # Create a temporary directory for the Whoosh index\n",
    "        self.index_dir = tempfile.mkdtemp()\n",
    "        # Define the schema: an ID and the chunk content.\n",
    "        schema = Schema(doc_id=ID(stored=True), content=TEXT(stored=True))\n",
    "        self.ix = create_in(self.index_dir, schema)\n",
    "\n",
    "    def index(self, chunks: List[Dict[str, str]]):\n",
    "        writer = self.ix.writer()\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            writer.add_document(doc_id=str(i), content=chunk[\"content\"])\n",
    "        writer.commit()\n",
    "\n",
    "    def search(self, query: str, top_k: int = 3) -> List[Dict[str, any]]:\n",
    "        results = []\n",
    "        with self.ix.searcher() as searcher:\n",
    "            parser = QueryParser(\"content\", schema=self.ix.schema)\n",
    "            parsed_query = parser.parse(query)\n",
    "            hits = searcher.search(parsed_query, limit=top_k)\n",
    "            for hit in hits:\n",
    "                results.append({\"text\": hit[\"content\"], \"score\": hit.score})\n",
    "        return results\n",
    "\n",
    "    def cleanup(self):\n",
    "        # Delete the temporary index directory\n",
    "        shutil.rmtree(self.index_dir)\n",
    "\n",
    "\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class FaissRetriever:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.doc_embeddings = None\n",
    "        self.chunk_texts = None\n",
    "\n",
    "    def index(self, chunks: List[Dict[str, str]]):\n",
    "        # Store the texts of each chunk\n",
    "        self.chunk_texts = [chunk[\"content\"] for chunk in chunks]\n",
    "        # Compute embeddings for each chunk\n",
    "        self.doc_embeddings = self.embedder.encode(self.chunk_texts, convert_to_numpy=True)\n",
    "        dimension = self.doc_embeddings.shape[1]\n",
    "        # Create an index (here using inner product; you can also use L2)\n",
    "        self.index = faiss.IndexFlatIP(dimension)\n",
    "        # Normalize embeddings if using inner product similarity\n",
    "        faiss.normalize_L2(self.doc_embeddings)\n",
    "        self.index.add(self.doc_embeddings)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 3) -> List[Dict[str, any]]:\n",
    "        # Compute query embedding\n",
    "        q_embed = self.embedder.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_embed)\n",
    "        distances, indices = self.index.search(q_embed, top_k)\n",
    "        results = []\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            results.append({\n",
    "                \"text\": self.chunk_texts[idx],\n",
    "                \"score\": float(dist)\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "def tfidf_search(chunks: List[Dict[str, str]], query: str, top_k: int = 3) -> List[Dict[str, any]]:\n",
    "    docs = [c[\"content\"] for c in chunks]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    doc_vectors = vectorizer.fit_transform(docs)\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vec, doc_vectors)[0]\n",
    "    ranked_idx = np.argsort(scores)[::-1]\n",
    "    results = []\n",
    "    for idx in ranked_idx[:top_k]:\n",
    "        results.append({\"text\": docs[idx], \"score\": float(scores[idx])})\n",
    "    return results\n",
    "\n",
    "def bm25_search(chunks: List[Dict[str, str]], query: str, top_k: int = 3) -> List[Dict[str, any]]:\n",
    "    docs = [c[\"content\"] for c in chunks]\n",
    "    tokenized_docs = [tokenizer.tokenize(doc) for doc in docs]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    ranked_idx = np.argsort(scores)[::-1]\n",
    "    results = []\n",
    "    for idx in ranked_idx[:top_k]:\n",
    "        results.append({\"text\": docs[idx], \"score\": float(scores[idx])})\n",
    "    return results\n",
    "\n",
    "\n",
    "# use WOOSH and FIASS too "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA MODELS LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_qa_pipelines():\n",
    "\n",
    "    return {\n",
    "        \"jasu_legalbert\": pipeline(\n",
    "            \"question-answering\",\n",
    "            model=\"Jasu/bert-finetuned-squad-legalbert\",\n",
    "            tokenizer=\"Jasu/bert-finetuned-squad-legalbert\"\n",
    "        ),\n",
    "        \n",
    "        \"nlpaueb_legalbert\": pipeline(\n",
    "            \"question-answering\",\n",
    "            model=\"nlpaueb/legal-bert-base-uncased\",\n",
    "            tokenizer=\"nlpaueb/legal-bert-base-uncased\"\n",
    "        ),\n",
    "\n",
    "        \"atharvamundada_bertlarge_legal\": pipeline(\n",
    "            \"question-answering\",\n",
    "            model=\"atharvamundada99/bert-large-question-answering-finetuned-legal\",\n",
    "            tokenizer=\"atharvamundada99/bert-large-question-answering-finetuned-legal\"\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNING THE RETRIEVAL & QA EXPERIMENT & EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whoosh =  WhooshRetriever()\n",
    "faiss =  FaissRetriever()\n",
    "\n",
    "def run_experiments(gold_data,\n",
    "                    processed_docs,\n",
    "                    qa_models,\n",
    "                    retrievers,\n",
    "                    top_k = 3):\n",
    "    \n",
    "    \"\"\"\n",
    "    For each agreement (from the gold standard), retrieve top chunks using TF-IDF and BM25,\n",
    "    run each QA model on the retrieved chunks for each question,\n",
    "    and store the best predicted answer.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for agreement_id, qa_pairs in tqdm(gold_data.items(), desc=\"Processing Agreements\"):\n",
    "\n",
    "        if agreement_id not in processed_docs:\n",
    "            print(f\"Warning: No processed chunks for {agreement_id}\")\n",
    "            continue\n",
    "\n",
    "        chunks = processed_docs[agreement_id]\n",
    "\n",
    "        # For better retrieval, we further chunk each section using our improved method.\n",
    "        chunks = further_chunk_sections(chunks, max_tokens=MAX_CHUNK_TOKENS)\n",
    "\n",
    "        # # Retrieve using the 4 methods - re-indexing chunks\n",
    "        # tfidf_results = tfidf_search(chunks, query=\"\", top_k=top_k)  # query will be overwritten per question\n",
    "        # bm25_results = bm25_search(chunks, query=\"\", top_k=top_k)\n",
    "        # whoosh_results = whoosh.search()\n",
    "        # faiss_results = FaissRetriever.search()\n",
    "\n",
    "        # For each retriever, we re-index the chunks\n",
    "        for retriever_name, retriever in retrievers.items():\n",
    "            # For retrievers implemented as classes, call .index(chunks)\n",
    "            retriever.index(chunks)\n",
    "                \n",
    "\n",
    "            # For each Q&A pair in the agreement\n",
    "            for qa in tqdm(qa_pairs, desc=f\"QA pairs for {agreement_id} ({retriever_name})\", leave=False):\n",
    "                question = qa[\"question\"]\n",
    "                gold_answer = qa[\"answer\"]\n",
    "                start_time = time.time()  # record start time for Q&A processing\n",
    "                \n",
    "                # Use the current retriever to get top_k chunks based on the question.\n",
    "                retrieved = retriever.search(query=question, top_k=top_k)\n",
    "\n",
    "                best_answer = \"\"\n",
    "                best_score = -1.0\n",
    "\n",
    "                # For each QA model, run the QA pipeline over the retrieved chunks.\n",
    "                for model_name, qa_pipeline in qa_models.items():\n",
    "                    for item in retrieved:\n",
    "\n",
    "                        try:\n",
    "                            pred = qa_pipeline({\n",
    "                                \"question\": question,\n",
    "                                \"context\": item[\"text\"] \n",
    "                            })\n",
    "                        \n",
    "                            if pred[\"score\"] > best_score:\n",
    "                                best_score = pred[\"score\"]\n",
    "                                best_answer = pred[\"answer\"]\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"QA error ({retriever_name}, {model_name}): {e}\")\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                results.append({\n",
    "                    \"retriever\": retriever_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"agreement_id\": agreement_id,\n",
    "                    \"question\": question,\n",
    "                    \"gold_answer\": gold_answer,\n",
    "                    \"pred_answer\": best_answer,\n",
    "                    \"confidence\": best_score,\n",
    "                    \"time\": elapsed_time\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "##############################\n",
    "#EVALUATION METRICS\n",
    "##############################\n",
    "\n",
    "def normalize_text(s):\n",
    "\n",
    "    s = s.lower().strip()\n",
    "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def tokenize_text(s):\n",
    "\n",
    "    return normalize_text(s).split()\n",
    "\n",
    "\n",
    "def compute_exact_match(pred, gold):\n",
    "\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def compute_token_level_f1(pred, gold):\n",
    "\n",
    "    pred_tokens = tokenize_text(pred)\n",
    "    gold_tokens = tokenize_text(gold)\n",
    "    common = 0\n",
    "    gold_counts = {}\n",
    "\n",
    "    for w in gold_tokens:\n",
    "        gold_counts[w] = gold_counts.get(w, 0) + 1\n",
    "\n",
    "    pred_counts = {}\n",
    "\n",
    "    for w in pred_tokens:\n",
    "        pred_counts[w] = pred_counts.get(w, 0) + 1\n",
    "\n",
    "    for w in gold_counts:\n",
    "        if w in pred_counts:\n",
    "            common += min(gold_counts[w], pred_counts[w])\n",
    "\n",
    "    if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = common / len(pred_tokens)\n",
    "    recall = common / len(gold_tokens)\n",
    "    return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "def compute_partial_f1(pred: str, gold: str) -> float:\n",
    "\n",
    "    pred_tokens = set(tokenize_text(pred))\n",
    "    gold_tokens = tokenize_text(gold)\n",
    "\n",
    "    if not gold_tokens:\n",
    "        return 1.0 if not pred_tokens else 0.0\n",
    "    \n",
    "    matches = sum(1 for w in gold_tokens if w in pred_tokens)\n",
    "    return matches / len(gold_tokens)\n",
    "\n",
    "\n",
    "def evaluate_all(results: List[Dict[str, str]]) -> Dict:\n",
    "\n",
    "    from collections import defaultdict\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    grouped = defaultdict(lambda: {\"preds\": [], \"golds\": []})\n",
    "\n",
    "    for res in results:\n",
    "        key = (res[\"retriever\"], res[\"model\"])\n",
    "        grouped[key][\"preds\"].append(res[\"pred_answer\"])\n",
    "        grouped[key][\"golds\"].append(res[\"gold_answer\"])\n",
    "    final = {}\n",
    "\n",
    "    for key, data in grouped.items():\n",
    "\n",
    "        ems = [compute_exact_match(p, g) for p, g in zip(data[\"preds\"], data[\"golds\"])]\n",
    "        f1s = [compute_token_level_f1(p, g) for p, g in zip(data[\"preds\"], data[\"golds\"])]\n",
    "        pf1s = [compute_partial_f1(p, g) for p, g in zip(data[\"preds\"], data[\"golds\"])]\n",
    "        avg_em = np.mean(ems)\n",
    "        avg_f1 = np.mean(f1s)\n",
    "        avg_pf1 = np.mean(pf1s)\n",
    "        rouge_scores = rouge.compute(predictions=data[\"preds\"], references=data[\"golds\"])\n",
    "        avg_rouge_l = rouge_scores[\"rougeL\"]\n",
    "        P, R, F = bertscore_score(data[\"preds\"], data[\"golds\"], lang=\"en\")\n",
    "        avg_bertscore = float(torch.mean(F))\n",
    "        \n",
    "        final[key] = {\n",
    "            \"Exact Match\": avg_em,\n",
    "            \"F1\": avg_f1,\n",
    "            \"Partial F1\": avg_pf1,\n",
    "            \"ROUGE-L\": avg_rouge_l,\n",
    "            \"BERTScore\": avg_bertscore\n",
    "        }\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    # 1. Preprocess all main agreement PDFs \n",
    "    process_all_agreement_pdfs()\n",
    "    \n",
    "    # 2. Build gold standard from Q&A PDFs\n",
    "    build_gold_standard()\n",
    "    \n",
    "    # 3. Load gold standard Q&A\n",
    "    gold_data = None\n",
    "    with open(GOLD_STANDARD_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        gold_data = json.load(f)\n",
    "    \n",
    "    # 4. Load processed (chunked) main agreement documents\n",
    "    processed_docs = {}\n",
    "    for json_file in PROCESSED_DIR.glob(\"Agreement_*.json\"):\n",
    "        agreement_id = json_file.stem\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            processed_docs[agreement_id] = json.load(f)\n",
    "    \n",
    "    # 5. Load QA pipelines\n",
    "    qa_models = load_qa_pipelines()\n",
    "\n",
    "\n",
    "\n",
    "    # 4) Initialize all retrievers.\n",
    "    # For TF-IDF and BM25, we assume you have function-based retrievals wrapped in simple objects.\n",
    "    class TfidfRetrieverWrapper:\n",
    "        def index(self, chunks: List[Dict[str, str]]):\n",
    "            # No persistent index is needed; this function call is a no-op.\n",
    "            self.chunks = chunks\n",
    "        def search(self, query: str, top_k: int = 3) -> List[Dict[str, any]]:\n",
    "            return tfidf_search(self.chunks, query, top_k=top_k)\n",
    "    \n",
    "    class BM25RetrieverWrapper:\n",
    "        def index(self, chunks: List[Dict[str, str]]):\n",
    "            self.chunks = chunks\n",
    "        def search(self, query: str, top_k: int = 3) -> List[Dict[str, any]]:\n",
    "            return bm25_search(self.chunks, query, top_k=top_k)\n",
    "    \n",
    "    # Instantiate wrappers and your Whoosh and FAISS retrievers.\n",
    "    tfidf_retriever = TfidfRetrieverWrapper()\n",
    "    bm25_retriever = BM25RetrieverWrapper()\n",
    "    whoosh_retriever = WhooshRetriever()  \n",
    "    faiss_retriever = FaissRetriever()    \n",
    "    \n",
    "    # Build the dictionary of retrievers.\n",
    "    retrievers = {\n",
    "        \"tfidf\": tfidf_retriever,\n",
    "        \"bm25\": bm25_retriever,\n",
    "        \"whoosh\": whoosh_retriever,\n",
    "        \"faiss\": faiss_retriever\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    # 6. Run experiments\n",
    "    results = run_experiments(gold_data, processed_docs, qa_models, retrievers, top_k=3)    \n",
    "\n",
    "        # Save detailed results to CSV.\n",
    "    df_detailed = pd.DataFrame(results)\n",
    "    detailed_csv_path = BASE_DIR / \"detailed_results.csv\"\n",
    "    df_detailed.to_csv(detailed_csv_path, index=False)\n",
    "    print(f\"Detailed results saved to {detailed_csv_path}\")\n",
    "\n",
    "    # 7. Evaluate\n",
    "    metrics = evaluate_all(results)\n",
    "    \n",
    "    # Convert aggregated metrics dictionary into a DataFrame.\n",
    "    metric_rows = []\n",
    "    for (retriever, model), scores in metrics.items():\n",
    "        row = {\"Retriever\": retriever, \"Model\": model}\n",
    "        row.update(scores)\n",
    "        metric_rows.append(row)\n",
    "    df_agg = pd.DataFrame(metric_rows)\n",
    "    \n",
    "    # Save aggregated metrics to CSV.\n",
    "    agg_csv_path = BASE_DIR / \"aggregated_metrics.csv\"\n",
    "    df_agg.to_csv(agg_csv_path, index=False)\n",
    "    print(f\"Aggregated metrics saved to {agg_csv_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------------------\n",
    "    # Chart 1: Compare QA Models (averaged over retrieval methods) for all metrics.\n",
    "    # For each metric, we average over retrievers per model.\n",
    "    models_avg = df_agg.groupby(\"Model\").mean().reset_index()\n",
    "    # Create a grouped bar chart where x-axis = metric, and bars = each model's average value.\n",
    "    metrics_list = [\"Exact Match\", \"F1\", \"Partial F1\", \"ROUGE-L\", \"BERTScore\", \"Avg Time\", \"Avg Confidence\"]\n",
    "\n",
    "    # Reshape the data for plotting:\n",
    "    df_models = pd.melt(models_avg, id_vars=[\"Model\"], value_vars=metrics_list,\n",
    "                        var_name=\"Metric\", value_name=\"Value\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric in metrics_list:\n",
    "        subset = df_models[df_models[\"Metric\"] == metric]\n",
    "        plt.bar(subset[\"Model\"] + \" (\" + metric + \")\", subset[\"Value\"])\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Aggregated Metrics per QA Model (Averaged over Retrievers)\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"chart1.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Chart 2: For each QA model, plot a bar chart showing metrics per retriever.\n",
    "    models = df_agg[\"Model\"].unique()\n",
    "    for model in models:\n",
    "        subset = df_agg[df_agg[\"Model\"] == model]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        x = np.arange(len(subset))  # each retrieval method for this model\n",
    "        width = 0.1\n",
    "        # Plot one bar per metric.\n",
    "        for i, metric in enumerate(metrics_list):\n",
    "            plt.bar(x + i * width, subset[metric], width, label=metric)\n",
    "        plt.xticks(x + width * (len(metrics_list) / 2), subset[\"Retriever\"])\n",
    "        plt.title(f\"Metrics per Retriever for QA Model: {model}\")\n",
    "        plt.xlabel(\"Retriever\")\n",
    "        plt.ylabel(\"Metric Value\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"chart2.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Chart 3: Compare QA Models against Retrieval Methods (using average F1 as example).\n",
    "    # For each retrieval method, plot a grouped bar chart of average F1 for each model.\n",
    "    # Define the list of metrics you want to display.\n",
    "    metrics_list = [\"Exact Match\", \"F1\", \"Partial F1\", \"ROUGE-L\", \"BERTScore\", \"Avg Time\", \"Avg Confidence\"]\n",
    "\n",
    "    # Determine the layout: here we use 2 columns.\n",
    "    n_metrics = len(metrics_list)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols  # ceiling division\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 4))\n",
    "    axes = axes.flatten()  # Flatten in case it's a 2D array\n",
    "\n",
    "    for i, metric in enumerate(metrics_list):\n",
    "        # Pivot the aggregated DataFrame so that rows = Retriever and columns = Model.\n",
    "        pivot = df_agg.pivot(index=\"Retriever\", columns=\"Model\", values=metric)\n",
    "        pivot.plot(kind=\"bar\", ax=axes[i])\n",
    "        axes[i].set_title(f\"Average {metric}: QA Models vs. Retrieval Methods\")\n",
    "        axes[i].set_xlabel(\"Retriever\")\n",
    "        axes[i].set_ylabel(metric)\n",
    "        axes[i].legend(title=\"Model\")\n",
    "        axes[i].set_xticklabels(pivot.index, rotation=0)\n",
    "\n",
    "    # Remove any unused subplots.\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"chart3.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ---------------------------\n",
    "    # Chart 4: Line graph comparing time per QA example (detailed results) by retriever-model combination.\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for (retriever, model), subset in df_detailed.groupby([\"retriever\", \"model\"]):\n",
    "        label = f\"{retriever}-{model}\"\n",
    "        plt.plot(subset.index, subset[\"time\"], marker=\"o\", label=label)\n",
    "    plt.title(\"Time per QA Example by Retriever-Model Combination\")\n",
    "    plt.xlabel(\"Example Index\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"chart4.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Compute instance-level F1 for each row.\n",
    "    def compute_f1_for_row(row):\n",
    "        return compute_token_level_f1(row[\"pred_answer\"], row[\"gold_answer\"])\n",
    "\n",
    "    df_detailed['F1'] = df_detailed.apply(compute_f1_for_row, axis=1)\n",
    "\n",
    "    # Compute instance-level semantic similarity using BERTScore.\n",
    "    # We compute BERTScore for all predictions against gold answers.\n",
    "    from bert_score import score as bertscore_score\n",
    "    P, R, F = bertscore_score(df_detailed['pred_answer'].tolist(),\n",
    "                            df_detailed['gold_answer'].tolist(),\n",
    "                            lang=\"en\")\n",
    "    df_detailed['BERTScore'] = F.tolist()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Scatter Plot 1: Semantic Similarity (BERTScore) vs Confidence\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(df_detailed['confidence'], df_detailed['BERTScore'], \n",
    "                alpha=0.6, color='green')\n",
    "    plt.xlabel(\"Confidence Score\")\n",
    "    plt.ylabel(\"BERTScore (Semantic Similarity)\")\n",
    "    plt.title(\"Scatter Plot: Semantic Similarity vs Confidence\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"scatter_sem_sim_conf.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Scatter Plot 2: F1 Score vs Confidence\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(df_detailed['confidence'], df_detailed['F1'], \n",
    "                alpha=0.6, color='blue')\n",
    "    plt.xlabel(\"Confidence Score\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"Scatter Plot: F1 Score vs Confidence\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"scatter_f1_conf.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Box Plot: Processing Time by Retriever-Model Combination\n",
    "\n",
    "    # Create a new column combining retriever and model.\n",
    "    df_detailed['Combination'] = df_detailed['retriever'] + \"-\" + df_detailed['model']\n",
    "    combinations = df_detailed['Combination'].unique()\n",
    "    data = [df_detailed[df_detailed['Combination'] == comb]['time'].values for comb in combinations]\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.boxplot(data, labels=combinations, patch_artist=True)\n",
    "    plt.xlabel(\"Retriever-Model Combination\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(\"Box Plot: Processing Time per Combination\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"boxplot_time.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # CDF Plot: Cumulative Distribution of Processing Time\n",
    "    sorted_time = np.sort(df_detailed['time'].values)\n",
    "    cdf = np.arange(len(sorted_time)) / float(len(sorted_time))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(sorted_time, cdf, marker=\".\", linestyle=\"none\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.title(\"CDF of Processing Time for Q&A Examples\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cdf_time.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Cleanup for Whoosh (delete temporary index)\n",
    "    whoosh_retriever.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# List of files to download.\n",
    "\n",
    "detailed_csv_path = BASE_DIR / \"detailed_results.csv\"\n",
    "agg_csv_path = BASE_DIR / \"aggregated_metrics.csv\"\n",
    "\n",
    "\n",
    "files_to_download = [\n",
    "    str(detailed_csv_path),\n",
    "    str(agg_csv_path),\n",
    "    \"chart1.png\",\n",
    "    \"chart2.png\",   # If you save one chart per model, you might need to loop over them.\n",
    "    \"chart3.png\",\n",
    "    \"chart3_all_metrics.png\",  # If you're saving all metrics as one image.\n",
    "    \"chart4.png\",\n",
    "    \"scatter_sem_sim_conf.png\",\n",
    "    \"scatter_f1_conf.png\",\n",
    "    \"boxplot_time.png\",\n",
    "    \"cdf_time.png\"\n",
    "]\n",
    "\n",
    "# Trigger download for each file.\n",
    "\n",
    "for f in files_to_download:\n",
    "    try:\n",
    "        files.download(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not download {f}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
