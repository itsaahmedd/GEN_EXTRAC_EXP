{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfUAD9NQzOnf",
        "outputId": "1420d60b-ff44-4773-d48b-e56a97990f5a"
      },
      "outputs": [],
      "source": [
        "pip install pandas rapidfuzz rouge seaborn torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "fZLsfvKPzMSq",
        "outputId": "d3d95774-d0fe-4a32-f5af-9bab40032cc6"
      },
      "outputs": [],
      "source": [
        "import re, string\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util  # for semantic similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge import Rouge\n",
        "import time\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())  # Should be True if a GPU is available\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")\n",
        "\n",
        "# ============================\n",
        "# 1. MOUNT GOOGLE DRIVE & LOAD DATASET\n",
        "# ============================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/Dissertation/cuad_qa_dataset.json\"\n",
        "print(\"Loading dataset from:\", dataset_path)\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "test_examples = data[\"test\"][:2000]\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Define Helper Functions (Normalization, Metrics, Chunking, TF-IDF Retrieval)\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Lowercase, remove punctuation, collapse whitespace.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def compute_em(prediction, gold):\n",
        "    \"\"\"Exact match: 1 if normalized prediction equals normalized gold, else 0.\"\"\"\n",
        "    return 1 if normalize_text(prediction) == normalize_text(gold) else 0\n",
        "\n",
        "def compute_f1(prediction, gold):\n",
        "    \"\"\"Token-level F1 between prediction and gold.\"\"\"\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    gold_tokens = normalize_text(gold).split()\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    if len(common) == 0:\n",
        "        return 0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(gold_tokens)\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def compute_partial_f1(prediction, gold):\n",
        "    \"\"\"Partial F1 as overlap tokens divided by average token count.\"\"\"\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    gold_tokens = normalize_text(gold).split()\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    if len(common) == 0:\n",
        "        return 0\n",
        "    avg_len = (len(pred_tokens) + len(gold_tokens)) / 2.0\n",
        "    return len(common) / avg_len\n",
        "\n",
        "# Semantic similarity via SentenceTransformers\n",
        "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "def compute_semantic_similarity(prediction, gold):\n",
        "    pred_emb = semantic_model.encode(normalize_text(prediction))\n",
        "    gold_emb = semantic_model.encode(normalize_text(gold))\n",
        "    return util.cos_sim(pred_emb, gold_emb).item()\n",
        "\n",
        "rouge = Rouge()\n",
        "def compute_rouge(prediction, gold):\n",
        "    pred_norm = normalize_text(prediction)\n",
        "    gold_norm = normalize_text(gold)\n",
        "    if not pred_norm or not gold_norm:\n",
        "        return 0\n",
        "    try:\n",
        "        scores = rouge.get_scores(pred_norm, gold_norm)\n",
        "        return scores[0]['rouge-l']['f']\n",
        "    except ValueError as e:\n",
        "        print(f\"ROUGE computation failed: {e}\")\n",
        "        return 0\n",
        "\n",
        "# Chunking: Split context into overlapping chunks (max_tokens default ~512)\n",
        "def chunk_text(text, tokenizer, max_tokens=512, overlap=50):\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        end = min(start + max_tokens, len(tokens))\n",
        "        chunk_tokens = tokens[start:end]\n",
        "        chunk = tokenizer.decode(chunk_tokens)\n",
        "        chunks.append(chunk)\n",
        "        start += max_tokens - overlap\n",
        "    return chunks\n",
        "\n",
        "# TF-IDF index building and retrieval functions:\n",
        "def build_tfidf_index(chunks):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def retrieve_top_k(query, vectorizer, tfidf_matrix, k=3):\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix)\n",
        "    top_indices = similarities.argsort()[0][-k:][::-1]\n",
        "    return top_indices, similarities[0][top_indices]\n",
        "\n",
        "def score_against_gold_list(prediction, gold_list):\n",
        "    \"\"\"\n",
        "    Given a prediction and a list of acceptable gold answers, compute the evaluation metrics\n",
        "    for each gold answer and return the maximum score for each metric.\n",
        "    \"\"\"\n",
        "    em_scores = [compute_em(prediction, gold) for gold in gold_list]\n",
        "    f1_scores = [compute_f1(prediction, gold) for gold in gold_list]\n",
        "    partial_f1_scores = [compute_partial_f1(prediction, gold) for gold in gold_list]\n",
        "    rouge_scores = [compute_rouge(prediction, gold) for gold in gold_list]\n",
        "    sem_sim_scores = [compute_semantic_similarity(prediction, gold) for gold in gold_list]\n",
        "\n",
        "    max_em = max(em_scores) if em_scores else 0\n",
        "    max_f1 = max(f1_scores) if f1_scores else 0\n",
        "    max_partial_f1 = max(partial_f1_scores) if partial_f1_scores else 0\n",
        "    max_rouge = max(rouge_scores) if rouge_scores else 0\n",
        "    max_sem_sim = max(sem_sim_scores) if sem_sim_scores else 0\n",
        "    return max_em, max_f1, max_partial_f1, max_rouge, max_sem_sim\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Define the QA Models\n",
        "# ============================================================================\n",
        "\n",
        "models = {\n",
        "    \"Jasu/legalbert\": pipeline(\"question-answering\",\n",
        "        model=AutoModelForQuestionAnswering.from_pretrained(\"Jasu/bert-finetuned-squad-legalbert\"),\n",
        "        tokenizer=AutoTokenizer.from_pretrained(\"Jasu/bert-finetuned-squad-legalbert\"),\n",
        "    ),\n",
        "    \"deepset/roberta-squad2\": pipeline(\"question-answering\",\n",
        "        model=AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\"),\n",
        "        tokenizer=AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\"),\n",
        "    ),\n",
        "    \"nlpaueb/legal-bert-base-uncased\": pipeline(\"question-answering\",\n",
        "        model=AutoModelForQuestionAnswering.from_pretrained(\"nlpaueb/legal-bert-base-uncased\"),\n",
        "        tokenizer=AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\"),\n",
        "    ),\n",
        "    \"atharvamundada99/bert-large-qa-legal\": pipeline(\"question-answering\",\n",
        "        model=AutoModelForQuestionAnswering.from_pretrained(\"atharvamundada99/bert-large-question-answering-finetuned-legal\"),\n",
        "        tokenizer=AutoTokenizer.from_pretrained(\"atharvamundada99/bert-large-question-answering-finetuned-legal\"),\n",
        "    )\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Process the CUAD Test Examples Using Baseline Questions and TF-IDF Retrieval\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "def extract_baseline_query(question):\n",
        "    # For baseline questions, extract the part after \"Details:\" if available.\n",
        "    if \"details:\" in question.lower():\n",
        "        return question.lower().split(\"details:\", 1)[1].strip()\n",
        "    else:\n",
        "        return question.strip()\n",
        "\n",
        "\n",
        "\n",
        "detailed_results = {}\n",
        "timing_results = {}\n",
        "use_chunking = True\n",
        "top_k_chunks = 3\n",
        "\n",
        "for model_name, qa_model in models.items():\n",
        "    timing_results[model_name] = []\n",
        "    detailed_results[model_name] = {}\n",
        "    print(f\"Running experiments for model: {model_name}\")\n",
        "    tokenizer = qa_model.tokenizer\n",
        "\n",
        "    for example in tqdm(test_examples, desc=\"Processing test examples\"):\n",
        "        example_id = example[\"id\"]\n",
        "        context_text = example[\"context\"]\n",
        "        query = extract_baseline_query(example[\"question\"])\n",
        "\n",
        "        # Use all gold answers from the \"answers\" field.\n",
        "        gold_answers = example[\"answers\"][\"text\"] if \"answers\" in example and example[\"answers\"][\"text\"] else []\n",
        "\n",
        "        # Split the context into chunks and build a TF-IDF index.\n",
        "        context_chunks = chunk_text(context_text, tokenizer, max_tokens=512, overlap=50)\n",
        "        vectorizer, tfidf_matrix = build_tfidf_index(context_chunks)\n",
        "\n",
        "        if use_chunking:\n",
        "            # Retrieve top-k relevant chunks for the current question\n",
        "            top_indices, sims = retrieve_top_k(query, vectorizer, tfidf_matrix, k=top_k_chunks)\n",
        "            # Concatenate the top-k chunks into one context string\n",
        "            retrieved_context = \" \".join([context_chunks[idx] for idx in top_indices])\n",
        "            # Optionally, ensure the retrieved context is truncated:\n",
        "            tokenized_context = tokenizer.encode(retrieved_context, add_special_tokens=True)\n",
        "            if len(tokenized_context) > 512:\n",
        "                tokenized_context = tokenized_context[:512]\n",
        "            used_context = tokenizer.decode(tokenized_context)\n",
        "        else:\n",
        "            used_context = context_text\n",
        "\n",
        "        start_time = time.time()\n",
        "        # Pass truncation=True to ensure the input is safely truncated\n",
        "        result = qa_model({\"question\": query, \"context\": used_context}, truncation=True)\n",
        "        end_time = time.time()\n",
        "        elapsed = end_time - start_time\n",
        "\n",
        "        timing_results[model_name].append(elapsed)\n",
        "\n",
        "        if isinstance(result, dict):\n",
        "            model_answer = result.get(\"answer\", \"\")\n",
        "            confidence = result.get(\"score\", None)\n",
        "            start_logit = result.get(\"start_logit\", None)\n",
        "            end_logit = result.get(\"end_logit\", None)\n",
        "        else:\n",
        "            model_answer = result\n",
        "            confidence = None\n",
        "            start_logit = None\n",
        "            end_logit = None\n",
        "\n",
        "        em, f1, partial_f1, rouge_score, semantic_sim = score_against_gold_list(model_answer, gold_answers)\n",
        "\n",
        "        detailed_results[model_name][example_id] = {\n",
        "            \"Question\": query,\n",
        "            \"Gold Answers\": gold_answers,\n",
        "            \"Model Answer\": model_answer,\n",
        "            \"Confidence\": confidence,\n",
        "            \"Start Logit\": start_logit,\n",
        "            \"End Logit\": end_logit,\n",
        "            \"EM\": em,\n",
        "            \"F1\": f1,\n",
        "            \"Partial F1\": partial_f1,\n",
        "            \"ROUGE\": rouge_score,\n",
        "            \"Semantic Similarity\": semantic_sim,\n",
        "            \"Used Context\": used_context\n",
        "        }\n",
        "\n",
        "# Compute average time per model\n",
        "avg_times = {model: np.mean(times) for model, times in timing_results.items()}\n",
        "print(\"Average time per model (in seconds):\", avg_times)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot average runtimes for each model:\n",
        "models_list_time = list(avg_times.keys())\n",
        "avg_times_list = [avg_times[model] for model in models_list_time]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=models_list_time, y=avg_times_list)\n",
        "plt.title(\"Average Runtime per Model (seconds)\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Avg Time (sec)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "with open(\"detailed_cuad_test_results.json\", \"w\") as outfile:\n",
        "    json.dump(detailed_results, outfile, indent=2)\n",
        "\n",
        "# ============================================================================\n",
        "# 5. Aggregate Metrics by Model\n",
        "# ============================================================================\n",
        "\n",
        "aggregated_metrics = {}\n",
        "\n",
        "for model_name, results in detailed_results.items():\n",
        "    ems, f1s, partial_f1s, rouges, sem_sims, confs = [], [], [], [], [], []\n",
        "    for ex_id, res in results.items():\n",
        "        ems.append(res[\"EM\"])\n",
        "        f1s.append(res[\"F1\"])\n",
        "        partial_f1s.append(res[\"Partial F1\"])\n",
        "        rouges.append(res[\"ROUGE\"])\n",
        "        sem_sims.append(res[\"Semantic Similarity\"])\n",
        "        confs.append(res[\"Confidence\"] if res[\"Confidence\"] is not None else 0)\n",
        "\n",
        "    aggregated_metrics[model_name] = {\n",
        "        \"Avg EM\": np.mean(ems),\n",
        "        \"Avg F1\": np.mean(f1s),\n",
        "        \"Avg Partial F1\": np.mean(partial_f1s),\n",
        "        \"Avg ROUGE\": np.mean(rouges),\n",
        "        \"Avg Semantic Similarity\": np.mean(sem_sims),\n",
        "        \"Avg Confidence\": np.mean(confs)\n",
        "    }\n",
        "\n",
        "with open(\"aggregated_cuad_test_results.json\", \"w\") as outfile:\n",
        "    json.dump(aggregated_metrics, outfile, indent=2)\n",
        "\n",
        "print(\"CUAD test experiment complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
